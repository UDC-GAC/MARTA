
* Basic usage:
--------------

To use, do (all in a terminal)

$> tar xzf exps.tar.gz
$> cd exps
Edit files:
- scripts/run_all_exps.sh
- scripts/data_preparer.sh
- scripts/subset_data.sh

and update the global variables at the beginning to correctly set the path to weka.jar.

Then, run (necessarily from the exps/ directory):

$> ./scripts/data_preparer.sh
$> ./scripts/run_all_exps.sh

and when you want to send me results, simply do
$> tar czf results_somename.tar.gz results
and send me this by email, as well as the all_results.csv file.

The columns in all_results.csv are:
{cusparse,cusp} name_of_feature_set classifier_algorithm minNumObj numFoldsPruning {rand1,rand2,rand3,rand4,rand5,all} number_of_mispredicted size_testing_set misprediction_ratio maximum_slowdown number_matrices_with_slowdown_greater1.25x number_matrices_with_slowdown_greater2x average_slowdown_pred_vs_best




* Advanced usage:
-----------------


1) Convert CSV to ARFF file (all data, beware to have unique name for columns in the CSV file):
java -cp ~/projects/Weka/weka-3-6-12/weka.jar weka.core.converters.CSVLoader data/CSV/k40c.sp-lnpprepared-csv.csv > data/ARFF/alldata_k40c.sp.arff

2) Create ARFF containing only useful features:
column layout:
matrix,group,n_rows,n_cols,nnz_tot,nnz_frac,nnz_min,nnz_max,nnz_mu,nnz_sigma,nnzb_tot,nnzb_min,nnzb_max,nnzb_mu,nnzb_sigma,snzb_min,snzb_max,snzb_mu,snzb_sigma,coord,type,cusparse.coo.conv_ok,cusparse.coo.t_h2d,cusparse.coo.t_conv,cusparse.coo.t_kernel,cusparse.coo.t_d2h,cusparse.ell.conv_ok,cusparse.ell.t_h2d,cusparse.ell.t_conv,cusparse.ell.t_kernel,cusparse.ell.t_d2h,cusparse.csr.conv_ok,cusparse.csr.t_h2d,cusparse.csr.t_conv,cusparse.csr.t_kernel,cusparse.csr.t_d2h,cusparse.hyb.conv_ok,cusparse.hyb.t_h2d,cusparse.hyb.t_conv,cusparse.hyb.t_kernel,cusparse.hyb.t_d2h,cusparse.hyb_mu.conv_ok,cusparse.hyb_mu.t_h2d,cusparse.hyb_mu.t_conv,cusparse.hyb_mu.t_kernel,cusparse.hyb_mu.t_d2h,cusparse.bsr.conv_ok,cusparse.bsr.t_h2d,cusparse.bsr.t_conv,cusparse.bsr.t_kernel,cusparse.bsr.t_d2h,cusp.coo.conv_ok,cusp.coo.t_h2d,cusp.coo.t_conv,cusp.coo.t_kernel,cusp.coo.t_d2h,cusp.ell.conv_ok,cusp.ell.t_h2d,cusp.ell.t_conv,cusp.ell.t_kernel,cusp.ell.t_d2h,cusp.csr.conv_ok,cusp.csr.t_h2d,cusp.csr.t_conv,cusp.csr.t_kernel,cusp.csr.t_d2h,cusp.hyb.conv_ok,cusp.hyb.t_h2d,cusp.hyb.t_conv,cusp.hyb.t_kernel,cusp.hyb.t_d2h,cusp.hyb.ellw,cusp.dia.conv_ok,cusp.dia.t_h2d,cusp.dia.t_conv,cusp.dia.t_kernel,cusp.dia.t_d2h,cusparse.gflops.cs-coo,cusparse.gflops.cs-ell,cusparse.gflops.cs-csr,cusparse.gflops.cs-hyb,cusparse.gflops.cs-hyb_mu,cusparse.gflops.cs-bsr,cusp.gflops.cu-coo,cusp.gflops.cu-ell,cusp.gflops.cu-csr,cusp.gflops.cu-hyb,cusp.gflops.cu-dia,best.overall.rep,best.cusparse.rep,best.cusp.rep

run, after all edits needed, if any,

$> ./scripts/data_preparer.sh

(note: edit the FORCE_REPLACE_FILES variable to 'yes' to replace existing files)

- to create a new dataset of features, look at the bottom of the script, and add two lines like:

create_dataset "cusparse" "extended2" "nnz_frac nnz_max nnz_mu nnz_sigma nnzb_mu nnzb_sigma snzb_mu snzb_sigma best.cusparse";
create_dataset "cusp" "extended2" "nnz_frac nnz_max nnz_mu nnz_sigma nnzb_mu nnzb_sigma snzb_mu snzb_sigma best.cusp";

where "extended2" is a unique name distinguishing this set of feature, and the 3rd argument is the list of feature this dataset shall contain, in any order.

Then, you need to edit run_all_exps.sh and add the name for the
feature set (e.g. "extended2") in the cases for the 'featSet' for
loop.


3) Run all experiments.

$> ./scripts/run_all_exps.sh

- parameters for the algorithms explored may be changed in the 'numObj' and 'maxLeaf' for loops. 'numObj' is the minNumOjb attribute, 'maxLeaf' is the numFoldsPruning attribute
- name of tree-based algorithms explored may be changed in the 'Algo' for loop condition, however only two parameters to be specified with -N and -M are supported for the moment.


* Author:
---------

@author Louis-Noel Pouchet <pouchet@cse.ohio-state.edu>
